{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product vertical from www.amazon.in. The product verticals to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting amazon page\n",
    "driver.get('https://www.amazon.in/')\n",
    "#Giving input\n",
    "search_item= input('Enter the product that has to be searched : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping search bar Xpath and clicking on search icon\n",
    "search_bar = driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')   \n",
    "search_bar.clear()                                               \n",
    "search_bar.send_keys(search_item)\n",
    "search_button = driver.find_element_by_xpath('/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_button.click()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product vertical has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all the Product URLS\n",
    "urls = []\n",
    "for i in range(0,3):\n",
    "    Page_urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-no-outline']\")#collecting urls of all the laptop\n",
    "    for i in Page_urls:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "        \n",
    "    #next button \n",
    "    nxt_btn=driver.find_element_by_xpath(\"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\") \n",
    "    url=nxt_btn.get_attribute('href')\n",
    "    driver.get(url)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making empty lists and scraping the required spots\n",
    "Product_name = []\n",
    "Brand_name= []\n",
    "Ratings = []\n",
    "No_Ratings = []\n",
    "Price = []\n",
    "Return = []\n",
    "Expected_Delivery = []\n",
    "Availability = []\n",
    "Other_Details = []\n",
    "\n",
    "#Start with for loop\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #Scraping data for product name\n",
    "    try:\n",
    "        prod=driver.find_element_by_xpath(\"//span[@id='productTitle']\")\n",
    "        Product_name.append(prod.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Product_name.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for brand name\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath(\"//div[@id='bylineInfo_feature_div']/div/a\")\n",
    "        Brand_name.append(brand.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Brand_name.append(\"-\")\n",
    "        \n",
    "        \n",
    "     #Scraping data for Ratings\n",
    "    try:\n",
    "        rat=driver.find_element_by_xpath(\"//span[@id='acrPopover']\")\n",
    "        Ratings.append(rat.get_attribute(\"title\"))   \n",
    "    except NoSuchElementException as e:\n",
    "        Ratings.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for No of Ratings\n",
    "    try:\n",
    "        no_rat=driver.find_element_by_xpath(\"//a[@id='acrCustomerReviewLink']/span\")\n",
    "        No_Ratings.append(no_rat.text)\n",
    "    except NoSuchElementException as e:\n",
    "        No_Ratings.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Price\n",
    "    try:\n",
    "        pri=driver.find_element_by_xpath(\"//span[@id='priceblock_ourprice']\")\n",
    "        Price.append(pri.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Price.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Return/Exchange\n",
    "    try:\n",
    "        ret=driver.find_element_by_xpath(\"//div[@data-name='RETURNS_POLICY']/span/div[2]/a\")\n",
    "        Return.append(ret.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Return.append(\"-\")\n",
    "        \n",
    "     \n",
    "    #Scraping data for Expected_Delivary\n",
    "    try:\n",
    "        delivary=driver.find_element_by_xpath(\"//div[@id='ddmDeliveryMessage']/b\")\n",
    "        Expected_Delivery.append(delivary.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Expected_Delivery.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Availability\n",
    "    try:\n",
    "        avai=driver.find_element_by_xpath(\"//div[@id='availability']/span\")\n",
    "        Availability.append(avai.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Availability.append(\"-\")\n",
    "        \n",
    "        \n",
    "    #Scraping data for Other_Details\n",
    "    try:\n",
    "        details=driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-vertical a-spacing-mini']\")\n",
    "        Other_Details.append(details.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Other_Details.append(\"-\")\n",
    "        \n",
    "        \n",
    "#DATA FRAMEING\n",
    "Laptop=pd.DataFrame({})\n",
    "Laptop['Name'] = Product_name\n",
    "Laptop['Brand'] = Brand_name\n",
    "Laptop['Rating'] = Ratings\n",
    "Laptop['No of Ratings'] = No_Ratings\n",
    "Laptop['Price'] = Price\n",
    "Laptop['Return'] = Return\n",
    "Laptop['Expected_Delivery'] = Expected_Delivery\n",
    "Laptop['Availability'] = Availability\n",
    "Laptop['Other_Details'] = Other_Details\n",
    "Laptop['Urls'] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the dataframe\n",
    "Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data to csv\n",
    "Laptop.to_csv('Amazon_{}.csv'.format(search_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser with specified url\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "# getting images.google.com\n",
    "url = \"https://images.google.com/\"\n",
    "#Creating empty list and giving search items as list and creating loop\n",
    "urls = []    \n",
    "data = []\n",
    "search_item = [\"fruits\", \"cars\", \"Machine Learning\"]\n",
    "for item in search_item:\n",
    "    driver.get(url)  \n",
    "    time.sleep(5)\n",
    "    search_bar = driver.find_element_by_tag_name(\"input\") #Xpath for search bar\n",
    "    \n",
    "    search_bar.send_keys(str(item))      #sending key word for search item\n",
    "    \n",
    "    search_button =driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\").click() #Clicking on search button\n",
    "    \n",
    "    # scrolling the web page to get more images\n",
    "    for _ in range(500):\n",
    "        driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "        \n",
    "        imgs = driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\")\n",
    "    img_url = []\n",
    "    for image in imgs:\n",
    "        source = image.get_attribute('src')\n",
    "        if source is not None:\n",
    "                if(source[0:4] == 'http'):\n",
    "                    img_url.append(source)\n",
    "    for i in img_url[:100]:\n",
    "        urls.append(i)\n",
    "                    \n",
    "for i in range(len(urls)):\n",
    "    if i >= 300:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 300))\n",
    "    response = requests.get(urls[i])\n",
    "\n",
    "    file = open(r\"C:\\Users\\nikit\\OneDrive\\Desktop\\DS Projects\\WS Selenium\"+str(i)+\".jpg\", \"wb\")\n",
    "\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have scraped all the images from google using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "item = input(\" Enter the name of Smartphone that has to be searched : \")\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") \n",
    "\n",
    "#get the web page with given url\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on login close icon\n",
    "login_X_btn = driver.find_element_by_xpath(\"//div[@class='_2QfC02']//button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving input key word to search bar\n",
    "serch_bar = driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "serch_bar.send_keys(item)\n",
    "\n",
    "srch_btn = driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "srch_btn.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching urls of phones coming on 1st page\n",
    "page1_urls = []\n",
    "urls = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    page1_urls.append(url.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page1_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smartphones = {}\n",
    "Smartphones[\"Brand\"] = []\n",
    "Smartphones[\"Phone name\"] = []\n",
    "Smartphones[\"Colour\"] = []\n",
    "Smartphones[\"RAM\"] = []\n",
    "Smartphones[\"Storage(ROM)\"] = []\n",
    "Smartphones[\"Primary Camera\"] = []\n",
    "Smartphones[\"Secondary Camera\"] = []\n",
    "Smartphones[\"Display Size\"] = []\n",
    "Smartphones[\"Display Resolution\"] = []\n",
    "Smartphones[\"Processor\"] = []\n",
    "Smartphones[\"Processor Cores\"] = []\n",
    "Smartphones[\"Battery Capacity\"] = []\n",
    "Smartphones[\"Price\"] = []\n",
    "Smartphones[\"URL\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from each url of page 1\n",
    "for url in page1_urls:\n",
    "    driver.get(url)                                                        \n",
    "    print(\"Scraping URL = \", url)\n",
    "    Smartphones['URL'].append(url)                                                          \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #Clicking on read more button\n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')     \n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception occured while moving to next page\")\n",
    "    \n",
    "    \n",
    "    #Scraping brand name of phone data\n",
    "    try:\n",
    "        brand_tags = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      \n",
    "        Smartphones[\"Brand\"].append(brand_tags.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Brand'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone name data\n",
    "    try:\n",
    "        name_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')     \n",
    "        Smartphones['Phone name'].append(name_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Phone name'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone color data\n",
    "    try:\n",
    "        color_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      \n",
    "        Smartphones['Colour'].append(color_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Colour'].append('-')\n",
    "     \n",
    "    \n",
    "    #Scraping RAM data\n",
    "    try:\n",
    "        ram_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                \n",
    "        Smartphones['RAM'].append(ram_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['RAM'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping ROM data\n",
    "    try:\n",
    "        rom_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')        \n",
    "        Smartphones['Storage(ROM)'].append(rom_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Storage(ROM)'].append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping Primary camera data\n",
    "    try:                                                                                    \n",
    "        pri_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        Smartphones['Primary Camera'].append(pri_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Primary Camera'].append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping secondary camera data\n",
    "    try:                                                                                    \n",
    "        sec_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if sec_tags != \"Secondary Camera\" : \n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        Smartphones['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Secondary Camera'].append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping Display size data \n",
    "    try:\n",
    "        disp_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_tags.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  \n",
    "        Smartphones['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Size'].append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping display resolution data\n",
    "    try:\n",
    "        dires_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if dires_tags.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_reso = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')    \n",
    "        Smartphones['Display Resolution'].append(disp_reso.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Display Resolution'].append('-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Scraping Processor data\n",
    "    try:\n",
    "        pro_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_tags.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   \n",
    "        Smartphones['Processor'].append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Processor'].append('-')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Scraping Processor core data    \n",
    "    try:                                                                                     \n",
    "        core_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_tags.text != \"Processor Core\" :\n",
    "            core_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_tags.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        Smartphones['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Processor Cores'].append('-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Scraping battery capacity data\n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                \n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_capa = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_tags = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_tags.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_capa = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')              \n",
    "        Smartphones['Battery Capacity'].append(bat_capa.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Battery Capacity'].append('-')\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Scraping Price data\n",
    "    try:\n",
    "        price_tags = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')      \n",
    "        Smartphones['Price'].append(price_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Smartphones['Price'].append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking lengths of all scraped data\n",
    "print(len(Smartphones[\"Brand\"]), len(Smartphones[\"Phone name\"]), len(Smartphones[\"Colour\"]), len(Smartphones[\"RAM\"]), len(Smartphones[\"Storage(ROM)\"]), len(Smartphones[\"Primary Camera\"]), len(Smartphones[\"Secondary Camera\"]), len(Smartphones[\"Display Size\"]), len(Smartphones[\"Display Resolution\"]), len(Smartphones[\"Processor\"]), len(Smartphones[\"Processor Cores\"]), len(Smartphones[\"Battery Capacity\"]), len(Smartphones[\"Price\"]), len(Smartphones['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Framing the data\n",
    "df = pd.DataFrame.from_dict(Smartphones)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving data to csv\n",
    "df.to_csv('Flipkart_{}.csv'.format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening google maps web page\n",
    "url = \"https://www.google.co.in/maps\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#Sending keyword for seach bar and search button\n",
    "city = input('Enter City name that has to be searched : ')\n",
    "search_bar = driver.find_element_by_id(\"searchboxinput\")                       \n",
    "search_bar.clear()                                                             \n",
    "time.sleep(2)\n",
    "search_bar.send_keys(city)                                                     \n",
    "search_btn = driver.find_element_by_id(\"searchbox-searchbutton\")              \n",
    "search_btn.click()                                                             \n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_str = driver.current_url\n",
    "    print(\"URL Extracted: \", url_str)\n",
    "    latitude_longitude = re.findall(r'@(.*)data',url_str)\n",
    "    if len(latitude_longitude):\n",
    "        lat_lng_list = latitude_longitude[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            latitude = lat_lng_list[0]\n",
    "            longitude = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(latitude, longitude))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have scraped the required information using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver.get('https://trak.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting xpath for funding deals\n",
    "fund_button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(fund_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists\n",
    "fund_deals = {}\n",
    "fund_deals['Date'] = []\n",
    "fund_deals['Startup Name'] = []\n",
    "fund_deals['Industry OR Vertical'] = []\n",
    "fund_deals['Sub-Vertical'] = []\n",
    "fund_deals['Location'] = []\n",
    "fund_deals['Investor'] = []\n",
    "fund_deals['Investment Type'] = []\n",
    "fund_deals['Amount(in USD)'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on tablepress\n",
    "for i in range(48,51):\n",
    "    driver.find_element_by_xpath('//div[@id=\"tablepress-{}_wrapper\"]/div/label/select/option[4]'.format(i)).click()\n",
    "\n",
    "    # Scraping data of Date\n",
    "    date_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for date in date_tags:\n",
    "        fund_deals['Date'].append(date.text)\n",
    "\n",
    "    # Scraping data of Startup Name\n",
    "    name_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for name in name_tags:\n",
    "        fund_deals['Startup Name'].append(name.text)\n",
    "    \n",
    "    # Scraping data of Industry OR Vertical\n",
    "    ind_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind_tags:\n",
    "        fund_deals['Industry OR Vertical'].append(n.text)\n",
    "    \n",
    "    # Scraping data of Sub-Vertical\n",
    "    sv_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for sv in sv_tags:\n",
    "        fund_deals['Sub-Vertical'].append(sv.text)\n",
    "\n",
    "    # Scraping data of Location\n",
    "    loc_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for loc in loc_tags:\n",
    "        fund_deals['Location'].append(loc.text)\n",
    "    \n",
    "    # Scraping data of Investor\n",
    "    inv_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for inv in inv_tags:\n",
    "        fund_deals['Investor'].append(inv.text)\n",
    "        \n",
    "    # Scraping data of Investment Type\n",
    "    invt_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for invt in invt_tags:\n",
    "        fund_deals['Investment Type'].append(invt.text)\n",
    "    \n",
    "    # Scraping data of Amount\n",
    "    amt_tags = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for amt in amt_tags:\n",
    "        fund_deals['Amount(in USD)'].append(amt.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Framimg the scraped data\n",
    "fund_df = pd.DataFrame(fund_deals)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving data to csv\n",
    "fund_df.to_csv(\"Trak_in.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the specified url\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "#searching for best laptop\n",
    "best_gam_lap = driver.find_element_by_xpath(\"//div[@class='listing_container']//ul//li[9]\").click()\n",
    "time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists\n",
    "lap_name = []\n",
    "ope_sys = []\n",
    "display = []\n",
    "processor = []\n",
    "memory = []\n",
    "weight = []\n",
    "dimensions = []\n",
    "graph_proc = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the data of laptop names\n",
    "name_tags = driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[1]\")\n",
    "for name in name_tags:\n",
    "    lap_name.append(name.text)\n",
    "    \n",
    "    \n",
    "#Scraping the data of operating system\n",
    "try:\n",
    "    os_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[3]//td[3]\")\n",
    "    for os in os_tags:\n",
    "        ope_sys.append(os.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of display\n",
    "try:\n",
    "    disp_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[4]//td[3]\")\n",
    "    for disp in disp_tags:\n",
    "        display.append(disp.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of Processor\n",
    "try:\n",
    "    pro_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[5]//td[3]\")\n",
    "    for pro in pro_tags:\n",
    "        processor.append(pro.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of memory\n",
    "try:\n",
    "    memo_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[6]//td[3]\")\n",
    "    for memo in memo_tags:\n",
    "        memory.append(memo.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of weight\n",
    "try:\n",
    "    wgt_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[7]//td[3]\")\n",
    "    for wgt in wgt_tags:\n",
    "        weight.append(wgt.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of dimensions\n",
    "try:\n",
    "    dim_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[8]//td[3]\")\n",
    "    for dim in dim_tags:\n",
    "        dimensions.append(dim.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of Graph processor\n",
    "try:\n",
    "    gra_tags = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']//tr[9]//td[3]\")\n",
    "    for gra in gra_tags:\n",
    "        graph_proc.append(gra.text)\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "\n",
    "#Scraping data of price\n",
    "try:\n",
    "    pri_tags = driver.find_elements_by_xpath(\"//td[@class='smprice']\")\n",
    "    for pri in pri_tags:\n",
    "        price.append(pri.text.replace('₹','Rs '))\n",
    "except NoSuchElementException:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Gaming_Laptop=pd.DataFrame({})\n",
    "Gaming_Laptop['Laptop Name'] = lap_name\n",
    "Gaming_Laptop['Operating system'] = ope_sys\n",
    "Gaming_Laptop['Display'] = display\n",
    "Gaming_Laptop['Processor'] = processor\n",
    "Gaming_Laptop['Memory'] = memory\n",
    "Gaming_Laptop['Weight'] = weight\n",
    "Gaming_Laptop['Dimensions'] = dimensions\n",
    "Gaming_Laptop['Graphical Processor'] = graph_proc\n",
    "Gaming_Laptop['Price'] = price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Gaming_Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataset to csv\n",
    "Gaming_Laptop.to_csv(\"Gaming_laptops.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "#get the specified url\n",
    "url = \"https://www.forbes.com/?sh=69e6b8c92254\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#clicking the explore button\n",
    "button = driver.find_element_by_xpath(\"//button[@class='icon--hamburger']\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#select billionaire  \n",
    "bill = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "bill.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#select world billionaire  \n",
    "world_bill= driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "world_bill.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping required data from the web page\n",
    "#Creating empty lists\n",
    "Rank = []\n",
    "Person_Name = []\n",
    "total_net_worth = []\n",
    "Age = []\n",
    "citizenship = []\n",
    "Source = []\n",
    "industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    #Scraping the data of rank\n",
    "    rank_tags= driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for rank in rank_tags:\n",
    "        Rank.append(rank.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping the data of names\n",
    "    name_tags= driver.find_elements_by_xpath(\"//div[@class='personName']//div\")\n",
    "    for name in name_tags:\n",
    "        Person_Name.append(name.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of age\n",
    "    age_tags= driver.find_elements_by_xpath(\"//div[@class='age']//div\")\n",
    "    for age in age_tags:\n",
    "        Age.append(age.text)   \n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of citizenship\n",
    "    cit_tags= driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for cit in cit_tags:\n",
    "        citizenship.append(cit.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of source of income\n",
    "    sour_tags= driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "    for sour in sour_tags:\n",
    "        Source.append(sour.text)    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data of Industry\n",
    "    ind_tags= driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "    for ind in ind_tags:\n",
    "        industry.append(ind.text)\n",
    "        \n",
    "        \n",
    "    #scraping data of net_worth of billionaire \n",
    "    net_tags= driver.find_elements_by_xpath(\"//div[@class='netWorth']//div[1]\")\n",
    "    for net in net_tags:\n",
    "        total_net_worth.append(net.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Clicking on next button\n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath(\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "#Scraping data of net worth\n",
    "Net_Worth = []\n",
    "for i in range(0,len(total_net_worth),2):\n",
    "    Net_Worth.append(total_net_worth[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Framing data\n",
    "Billionaires=pd.DataFrame({})\n",
    "Billionaires['Rank'] = Rank\n",
    "Billionaires['Names'] = Person_Name\n",
    "Billionaires['Net Worth'] = Net_Worth\n",
    "Billionaires['Age'] = Age\n",
    "Billionaires['Citizenship'] = citizenship\n",
    "Billionaires['Source'] = Source\n",
    "Billionaires['Industry'] = industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing dataframe\n",
    "Billionaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataset to csv\n",
    "Billionaires.to_csv(\"Billionaries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening youtube\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding element for search bar\n",
    "search_bar = driver.find_element_by_xpath('//input[@class=\"ytd-searchbox\"]')\n",
    "search_bar.send_keys(\"GOT\")  #entering Video name\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on search button\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")  \n",
    "search_btn.click()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on first video\n",
    "link_click = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 time we scroll down to generate more Comments\n",
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the data of comments\n",
    "cm_tags = driver.find_elements_by_id(\"content-text\")\n",
    "for cm in cm_tags:\n",
    "    if cm.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(cm.text)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data of time when comment was posted\n",
    "tm_tags = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for tm in tm_tags:\n",
    "    Time.append(tm.text)\n",
    "\n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the data of comment likes\n",
    "like_tags = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for like in like_tags:\n",
    "    Likes.append(like.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "Youtube=pd.DataFrame({})\n",
    "Youtube['Comments'] = comments\n",
    "Youtube['Comment_time'] = comment_time\n",
    "Youtube['Comment upvotes'] = No_of_Likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing dataframe\n",
    "Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataset to csv\n",
    "Youtube.to_csv(\"Youtube.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "#get the web page with given url\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the location search bar\n",
    "search_loc = driver.find_element_by_id('search-input-field')\n",
    "# write Lonodn in search bar\n",
    "search_loc.send_keys(\"London\")\n",
    "time.sleep(5)\n",
    "\n",
    "#select london\n",
    "london = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]/div')\n",
    "london.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# do click on search button\n",
    "search_btn = driver.find_element_by_id('search-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping the requered informations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    #fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "    #fetching distance from city centre\n",
    "    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "    #fetch privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "    #fetching dorms from price\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "            #fetching facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "    #lets fetch url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    #lets click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    #fetching ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    #fetching total reviews\n",
    "        \n",
    "    try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "        #fetch overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    #fetch property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe\n",
    "dff = pd.DataFrame({})\n",
    "dff['Hostel_Name'] = hostel_name\n",
    "dff['Distance fron city centre'] = distance\n",
    "dff['Ratings'] = rating\n",
    "dff['Total_reviews'] = reviews\n",
    "dff['Overall Reviews'] = over_all\n",
    "dff['Privates from price'] = pvt_prices\n",
    "dff['Dorms from price'] = dorms_price\n",
    "dff['Facilities'] = facilities[:83]\n",
    "dff['Description'] = description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataset to csv\n",
    "dff.to_csv(\"London_Hostels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
